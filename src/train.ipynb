{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematic explanations : https://machinelearnia.com/regression-lineaire-simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force external files reloading\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import myvars as v\n",
    "\n",
    "# Importing modules\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "from IPython import display as dp\n",
    "import time\n",
    "\n",
    "# Location of the dataset\n",
    "DATA_LOCATION = '../data/data.csv'\n",
    "\n",
    "# Creating a fresh theta vars files\n",
    "theta = v.init(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dataset existence\n",
    "if (os.path.exists(DATA_LOCATION) != True):\n",
    "\tprint('Dataset file doesn\\'t exists at ' + DATA_LOCATION)\n",
    "\traise KeyboardInterrupt()\n",
    "\n",
    "print('We assume that the dataset is correctly formated with valid data')\n",
    "# Openning data file and parsing it into 2 lists\n",
    "file = open(DATA_LOCATION, \"r\")\n",
    "reader = csv.reader(file)\n",
    "next(reader)\n",
    "km = []\n",
    "price = []\n",
    "for row in reader:\n",
    "\tkm.append(row[0])\n",
    "\tprice.append(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling and shaping matixes, then representing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting matrixes dimmensions infos\n",
    "m = len(km)\n",
    "# Creating empty numpy arrays then filling them with values from dataset\n",
    "x = np.empty((m, 1), float)\n",
    "i = 0\n",
    "for num in km:\n",
    "\tx[i, 0] = float(km[i])\n",
    "\ti += 1\n",
    "y = np.empty((m, 1), float)\n",
    "i = 0\n",
    "for num in price:\n",
    "\ty[i, 0] = float(price[i])\n",
    "\ti += 1\n",
    "\n",
    "print('x is a', x.shape, 'matrix filled with mileages values')\n",
    "print('y is a', y.shape, 'matrix filled with prices values')\n",
    "\n",
    "# Printing a graph reprensenting the dataset\n",
    "print('Data graph :')\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"Data representation\", fontsize=20)\n",
    "plt.xlabel(\"mileage (km)\")\n",
    "plt.ylabel(\"price ($)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preventing from overflows by normalizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization to keep 0 <= values <= 1 and preventing future overflows\n",
    "def normalize(nb):\n",
    "    return (nb - min(nb)) / (max(nb) - min(nb))\n",
    "\n",
    "backup = x\n",
    "x = normalize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding bias column and setting random theta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding bias column to x\n",
    "X = np.hstack((x, np.ones(x.shape)))\n",
    "print('X is a', x.shape, 'matrix filled with mileages values and a bias column')\n",
    "# Setting random initial theta values\n",
    "theta = np.random.randn(2, 1)\n",
    "print('Initial theta values (random), theta0 = ' + str(theta[0]) + ' $, theta1 = ' + str(theta[1]) + ' $/km')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBH, setting the formulae as F = theta0 + theta1 * x is complete garbage. theta0 and theta1 should be inverted\n",
    "# The model contains all predictions in a single matrix, here is the demonstration\n",
    "#\t\tF \t\t=\t\tX \t\t.\ttheta\t=\ttheta0 + theta1 * x\n",
    "#  [predict(0)\t   [x(0),\t1\n",
    "#\tpredict(1)\t\tx(1),\t1\t   [theta1\n",
    "#\tpredict(2)\t=\tx(2),\t1\t.\ttheta0]\n",
    "#\t...\t\t\t\t...\n",
    "#\tpredict(m)]\t\tx(m),\t1]\n",
    "def model(X, theta):\n",
    "\treturn X.dot(theta)\n",
    "\n",
    "# Difference = (model(X, theta) - y)\n",
    "# Average difference = 1 / m * sum(differences)\n",
    "# The cost function returns the mean squared error (average euclidean distance between prediction and value from dataset) divided by 2\n",
    "def cost_function(X, y, theta):\n",
    "\tm = len(y)\n",
    "\treturn 1/(2*m) * np.sum((model(X, theta) - y)**2)\n",
    "\n",
    "# Note : we are searching the theta values such as MSE with this vector is minimal\n",
    "# The gradient is the vector representing first derivative of the cost function, meaning that adjusting theta in this direction is minimizing MSE\n",
    "def grad(X, y, theta):\n",
    "\tm = len(y)\n",
    "\treturn 1/m * X.T.dot(model(X, theta) - y)\n",
    "\n",
    "LEARNING_TRESHOLD = 0.03 # ? Under this ratio, we stop the learning phase\n",
    "\n",
    "# Proper learning phase, \n",
    "def gradient_descent(X, y, theta, learning_rate, n_iterations):\n",
    "\t# Initializing MSE history matrix\n",
    "\tcost_history = []\n",
    "\t# For a (defined in next cell) number of iterations\n",
    "\tfor i in range(0, n_iterations):\n",
    "\n",
    "\t\t# * Uncomment this to show \"live\" learning\n",
    "\t\t# plt.scatter(x, y)\n",
    "\t\t# plt.title(\"Machine learning\", fontsize=20)\n",
    "\t\t# plt.xlabel(\"mileage (km, normalized)\")\n",
    "\t\t# plt.ylabel(\"price ($)\")\n",
    "\t\t# plt.plot(x, model(X, theta), c='r')\n",
    "\t\t# dp.clear_output(wait=True)\n",
    "\t\t# dp.display(plt.gcf())\n",
    "\t\t# time.sleep(0.0005)\n",
    "\t\t# plt.clf()\n",
    "\n",
    "\t\t# Correcting theta value by substracting a part of the gradient to move in a way to minimize MSE\n",
    "\t\t# Learning rate is not high because you don't want to got too far from where you were to prevent from boucing\n",
    "\t\t# \tfrom a \"side of point with minimal MSE\" to the other (look at a basic square function, if you are on the left side of the minimum you want to\n",
    "\t\t# \tgo to the other side but keep approaching iteration by iteration)\n",
    "\t\t# Learning rate too low would also result in a slow approch, which is not desired either. You need to try some values and see how it goes to adjust\n",
    "\t\tif (abs(grad(X, y, theta))[0] < abs(theta[0]) / 100 * LEARNING_TRESHOLD) and (abs(grad(X, y, theta))[1] < abs(theta[1]) / 100 * LEARNING_TRESHOLD):\n",
    "\t\t\tbreak # ? Stop if further iterations wouldn't improve the model more than slightly\n",
    "\t\ttheta -= learning_rate * grad(X, y, theta)\n",
    "\t\t# Adding current MSE to history\n",
    "\t\tcost_history.append(cost_function(X, y, theta))\n",
    "\t# Returning final adjusted value\n",
    "\treturn theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting learning variables and representing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times the program will iterate\n",
    "n_iterations = 2000\n",
    "# Multiplication factor of the MSE to prevent from over reacting\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Printing graph and initial function before learning\n",
    "print('Graph before learning :')\n",
    "plt.scatter(backup, y)\n",
    "plt.title(\"Before learning\", fontsize=20)\n",
    "plt.xlabel(\"mileage (km)\")\n",
    "plt.ylabel(\"price ($)\")\n",
    "plt.plot(backup, model(X, theta), c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning, saving and showing results, then reprensenting again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning phase\n",
    "theta_final, cost_history = gradient_descent(X, y, theta, learning_rate, n_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling a matrix with predictions (Predictions = X . theta)\n",
    "predictions = model(X, theta_final)\n",
    "\n",
    "# Reverting normalization to get back to initial values\n",
    "theta_final[0] = theta_final[0] / ((max(backup) - min(backup)) + min(backup))\n",
    "x = x * (max(backup) - min(backup)) + min(backup)\n",
    "# Theta file updating\n",
    "file = open(v.THETA_LOCATION, \"w\")\n",
    "file.write(str(theta_final[1]).replace('[', '').replace(']', '') + ',' + str(theta_final[0]).replace('[', '').replace(']', ''))\n",
    "file.close()\n",
    "# Final theta values report\n",
    "print('Final theta values, theta0 = ' + str(theta_final[1]) + ' $, theta1 = ' + str(theta_final[0]) + ' $/km')\n",
    "\n",
    "# Printing graph and function after learning\n",
    "print('Graph after learning :')\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"After learning\", fontsize=20)\n",
    "plt.xlabel(\"mileage (km)\")\n",
    "plt.ylabel(\"price ($)\")\n",
    "plt.plot(x, predictions, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representing cost function history (mean squared error evolution over iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representing learning curve / MSE over iterations\n",
    "print('Learning curve :')\n",
    "j = -1\n",
    "PLOT_RATIO = 15 # ? Cap ratio between min and max cost_history values\n",
    "for i in range (0, len(cost_history)):\n",
    "    if cost_history[i] > PLOT_RATIO * cost_history[-1]:\n",
    "        cost_history[i] = PLOT_RATIO * cost_history[-1]\n",
    "        j = i\n",
    "if j > -1:\n",
    "\tprint('First', j, 'values manually capped to', PLOT_RATIO * cost_history[-1])\n",
    "print('The learning has processed to', len(cost_history), 'iterations')\n",
    "plt.plot(range(len(cost_history)), cost_history)\n",
    "plt.title(\"Mean squared error evolution over iterations\\n\", fontsize=20)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating determination coefficient and printing it\n",
    "u = ((y - predictions) ** 2).sum()\n",
    "v = ((y - y.mean()) ** 2).sum()\n",
    "print('Determination cofficient :', 1 - u / v)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 2.7.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
